{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L5WD_VfeF-j"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import WhitespaceTokenizer, word_tokenize, TreebankWordTokenizer, TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDEbo1aNeOk7",
        "outputId": "c4c2b915-4b6a-4077-cc7c-75697dfa0b5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0Phpq2Le30-",
        "outputId": "b9618a8e-3df2-425a-c8e7-536e15d37529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "k_cUAN55fBoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"\n",
        "The quick brown fox jumps over the lazy dog! Don't stop learning. AI and ML are\n",
        "transforming the future. #ArtificialIntelligence @OpenAI.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "e1clbilXeVmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__9QFK3xe_lu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whitespace Tokenizer\n",
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n"
      ],
      "metadata": {
        "id": "kMY6tBz2eXyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation_tokens = word_tokenize(text)"
      ],
      "metadata": {
        "id": "dcEtFROfebxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n"
      ],
      "metadata": {
        "id": "15SXMxU5egjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweet Tokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "\n",
        "# Multi-Word Expression Tokenizer\n",
        "mwe_tokenizer = MWETokenizer([('Artificial', 'Intelligence'), ('machine', 'learning')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n"
      ],
      "metadata": {
        "id": "ttJGvveWfLFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "porter_stemmed = [porter_stemmer.stem(token) for token in punctuation_tokens]\n",
        "\n",
        "# Snowball Stemmer\n",
        "snowball_stemmer = SnowballStemmer('english')\n",
        "snowball_stemmed = [snowball_stemmer.stem(token) for token in punctuation_tokens]\n"
      ],
      "metadata": {
        "id": "L6OVm2bCfUe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(token) for token in punctuation_tokens]\n"
      ],
      "metadata": {
        "id": "GS9uAkQefXhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nTokenization:\")\n",
        "print(\"Whitespace Tokenizer:\", whitespace_tokens)\n",
        "print(\"Punctuation-based Tokenizer:\", punctuation_tokens)\n",
        "print(\"Treebank Tokenizer:\", treebank_tokens)\n",
        "print(\"Tweet Tokenizer:\", tweet_tokens)\n",
        "print(\"MWE Tokenizer:\", mwe_tokens)\n",
        "\n",
        "print(\"\\nStemming:\")\n",
        "print(\"Porter Stemmer:\", porter_stemmed)\n",
        "print(\"Snowball Stemmer:\", snowball_stemmed)\n",
        "\n",
        "print(\"\\nLemmatization:\")\n",
        "print(\"Lemmatized:\", lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL_1mt_gfbMN",
        "outputId": "5025b860-375a-4d89-d62c-5cc3d4e07c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "\n",
            "The quick brown fox jumps over the lazy dog! Don't stop learning. AI and ML are\n",
            "transforming the future. #ArtificialIntelligence @OpenAI.\n",
            "\n",
            "\n",
            "Tokenization:\n",
            "Whitespace Tokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog!', \"Don't\", 'stop', 'learning.', 'AI', 'and', 'ML', 'are', 'transforming', 'the', 'future.', '#ArtificialIntelligence', '@OpenAI.']\n",
            "Punctuation-based Tokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '!', 'Do', \"n't\", 'stop', 'learning', '.', 'AI', 'and', 'ML', 'are', 'transforming', 'the', 'future', '.', '#', 'ArtificialIntelligence', '@', 'OpenAI', '.']\n",
            "Treebank Tokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '!', 'Do', \"n't\", 'stop', 'learning.', 'AI', 'and', 'ML', 'are', 'transforming', 'the', 'future.', '#', 'ArtificialIntelligence', '@', 'OpenAI', '.']\n",
            "Tweet Tokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '!', \"Don't\", 'stop', 'learning', '.', 'AI', 'and', 'ML', 'are', 'transforming', 'the', 'future', '.', '#ArtificialIntelligence', '@OpenAI', '.']\n",
            "MWE Tokenizer: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '!', 'Do', \"n't\", 'stop', 'learning', '.', 'AI', 'and', 'ML', 'are', 'transforming', 'the', 'future', '.', '#', 'ArtificialIntelligence', '@', 'OpenAI', '.']\n",
            "\n",
            "Stemming:\n",
            "Porter Stemmer: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '!', 'do', \"n't\", 'stop', 'learn', '.', 'ai', 'and', 'ml', 'are', 'transform', 'the', 'futur', '.', '#', 'artificialintellig', '@', 'openai', '.']\n",
            "Snowball Stemmer: ['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazi', 'dog', '!', 'do', \"n't\", 'stop', 'learn', '.', 'ai', 'and', 'ml', 'are', 'transform', 'the', 'futur', '.', '#', 'artificialintellig', '@', 'openai', '.']\n",
            "\n",
            "Lemmatization:\n",
            "Lemmatized: ['The', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog', '!', 'Do', \"n't\", 'stop', 'learning', '.', 'AI', 'and', 'ML', 'are', 'transforming', 'the', 'future', '.', '#', 'ArtificialIntelligence', '@', 'OpenAI', '.']\n"
          ]
        }
      ]
    }
  ]
}